{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "647e39e2-e799-4f17-a135-8c53243be314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e652122-5615-4c0d-86d8-52d87f8f6c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Setting Spark log level to \"INFO\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:18:44.606 [Thread-3] INFO  org.apache.spark.resource.ResourceUtils - ==============================================================\n",
      "13:18:44.609 [Thread-3] INFO  org.apache.spark.resource.ResourceUtils - No custom resources configured for spark.driver.\n",
      "13:18:44.610 [Thread-3] INFO  org.apache.spark.resource.ResourceUtils - ==============================================================\n",
      "13:18:44.610 [Thread-3] INFO  org.apache.spark.SparkContext - Submitted application: Spark Mlflow\n",
      "13:18:44.656 [Thread-3] INFO  org.apache.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "13:18:44.671 [Thread-3] INFO  org.apache.spark.resource.ResourceProfile - Limiting resource is cpus at 1 tasks per executor\n",
      "13:18:44.672 [Thread-3] INFO  org.apache.spark.resource.ResourceProfileManager - Added ResourceProfile id: 0\n",
      "13:18:44.716 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing view acls to: root\n",
      "13:18:44.716 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing modify acls to: root\n",
      "13:18:44.716 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing view acls groups to: \n",
      "13:18:44.717 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing modify acls groups to: \n",
      "13:18:44.717 [Thread-3] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
      "13:18:44.903 [Thread-3] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7078. Attempting port 7079.\n",
      "13:18:44.906 [Thread-3] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7079. Attempting port 7080.\n",
      "13:18:44.909 [Thread-3] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7080. Attempting port 7081.\n",
      "13:18:44.919 [Thread-3] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 7081.\n",
      "13:18:44.946 [Thread-3] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker\n",
      "13:18:44.981 [Thread-3] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster\n",
      "13:18:45.004 [Thread-3] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "13:18:45.005 [Thread-3] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up\n",
      "13:18:45.007 [Thread-3] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat\n",
      "13:18:45.029 [Thread-3] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-bc21d5a4-0924-4b75-84e4-3cb979959647\n",
      "13:18:45.041 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 434.4 MiB\n",
      "13:18:45.055 [Thread-3] INFO  org.apache.spark.SparkEnv - Registering OutputCommitCoordinator\n",
      "13:18:45.093 [Thread-3] INFO  org.sparkproject.jetty.util.log - Logging initialized @2141ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "13:18:45.212 [Thread-3] INFO  org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "13:18:45.229 [Thread-3] INFO  org.sparkproject.jetty.server.Server - jetty-9.4.54.v20240208; built: 2024-02-08T19:42:39.027Z; git: cef3fbd6d736a21e7d541a5db490381d95a2047d; jvm 11.0.25+9\n",
      "13:18:45.259 [Thread-3] INFO  org.sparkproject.jetty.server.Server - Started @2308ms\n",
      "13:18:45.278 [Thread-3] WARN  org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "13:18:45.284 [Thread-3] INFO  org.sparkproject.jetty.server.AbstractConnector - Started ServerConnector@651d0010{HTTP/1.1, (http/1.1)}{0.0.0.0:4041}\n",
      "13:18:45.284 [Thread-3] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4041.\n",
      "13:18:45.298 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b3f07{/,null,AVAILABLE,@Spark}\n",
      "13:18:45.338 [Thread-3] INFO  org.apache.spark.scheduler.FairSchedulableBuilder - Fair scheduler configuration not found, created default pool: default, schedulingMode: FAIR, minShare: 0, weight: 1\n",
      "13:18:45.373 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint - Connecting to master spark://spark-master:7077...\n",
      "13:18:45.427 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory - Successfully created connection to spark-master/10.89.2.6:7077 after 30 ms (0 ms spent in bootstraps)\n",
      "13:18:45.544 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend - Connected to Spark cluster with app ID app-20250915131845-0004\n",
      "13:18:45.554 [Thread-3] WARN  org.apache.spark.util.Utils - Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 7079. Attempting port 7080.\n",
      "13:18:45.559 [Thread-3] WARN  org.apache.spark.util.Utils - Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 7080. Attempting port 7081.\n",
      "13:18:45.562 [Thread-3] WARN  org.apache.spark.util.Utils - Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 7081. Attempting port 7082.\n",
      "13:18:45.564 [Thread-3] INFO  org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7082.\n",
      "13:18:45.564 [Thread-3] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on spark-master 0.0.0.0:7082\n",
      "13:18:45.566 [Thread-3] INFO  org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "13:18:45.566 [Thread-3] INFO  org.apache.spark.storage.BlockManager - external shuffle service port = 7337\n",
      "13:18:45.571 [Thread-3] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, spark-master, 7082, None)\n",
      "13:18:45.573 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager spark-master:7082 with 434.4 MiB RAM, BlockManagerId(driver, spark-master, 7082, None)\n",
      "13:18:45.575 [Thread-3] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, spark-master, 7082, None)\n",
      "13:18:45.576 [Thread-3] INFO  org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, spark-master, 7082, None)\n",
      "13:18:45.822 [Thread-3] INFO  org.apache.spark.deploy.history.RollingEventLogFilesWriter - Logging events to file:/opt/spark/events/eventlog_v2_app-20250915131845-0004/events_1_app-20250915131845-0004.zstd\n",
      "13:18:45.937 [Thread-3] INFO  org.apache.spark.util.Utils - Using initial executors = 0, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "13:18:45.987 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@b3f07{/,null,STOPPED,@Spark}\n",
      "13:18:45.988 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a24a84c{/jobs,null,AVAILABLE,@Spark}\n",
      "13:18:45.989 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5fd0a64b{/jobs/json,null,AVAILABLE,@Spark}\n",
      "13:18:45.990 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2afbfcbd{/jobs/job,null,AVAILABLE,@Spark}\n",
      "13:18:45.992 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2845cadc{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "13:18:45.995 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@327290e0{/stages,null,AVAILABLE,@Spark}\n",
      "13:18:45.997 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56a10761{/stages/json,null,AVAILABLE,@Spark}\n",
      "13:18:45.998 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6948fc06{/stages/stage,null,AVAILABLE,@Spark}\n",
      "13:18:45.999 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13577807{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "13:18:46.000 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ee9e067{/stages/pool,null,AVAILABLE,@Spark}\n",
      "13:18:46.000 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@777a77af{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "13:18:46.001 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ad79430{/storage,null,AVAILABLE,@Spark}\n",
      "13:18:46.001 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3cdb6466{/storage/json,null,AVAILABLE,@Spark}\n",
      "13:18:46.002 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1466ff72{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "13:18:46.002 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ee2a915{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "13:18:46.003 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b445eb0{/environment,null,AVAILABLE,@Spark}\n",
      "13:18:46.004 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6169b0d7{/environment/json,null,AVAILABLE,@Spark}\n",
      "13:18:46.005 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@354bbe05{/executors,null,AVAILABLE,@Spark}\n",
      "13:18:46.006 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ee46fa{/executors/json,null,AVAILABLE,@Spark}\n",
      "13:18:46.011 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@198b85e4{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "13:18:46.012 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2aeaed35{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "13:18:46.012 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b555665{/executors/heapHistogram,null,AVAILABLE,@Spark}\n",
      "13:18:46.013 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@686e33dd{/executors/heapHistogram/json,null,AVAILABLE,@Spark}\n",
      "13:18:46.022 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2556eb75{/static,null,AVAILABLE,@Spark}\n",
      "13:18:46.022 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a9c3703{/,null,AVAILABLE,@Spark}\n",
      "13:18:46.025 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71a5600e{/api,null,AVAILABLE,@Spark}\n",
      "13:18:46.026 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49b2f5e3{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "13:18:46.026 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e22242f{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "13:18:46.033 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f4b5c4{/metrics/json,null,AVAILABLE,@Spark}\n",
      "13:18:46.035 [Thread-3] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Spark Mlflow\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd083e90-c831-4194-8c67-ee776503bd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:18:57.559 [Thread-3] INFO  org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "13:18:57.567 [Thread-3] INFO  org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/opt/spark/notebook/spark-warehouse'.\n",
      "13:18:57.586 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ebb81ab{/SQL,null,AVAILABLE,@Spark}\n",
      "13:18:57.587 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6daecd93{/SQL/json,null,AVAILABLE,@Spark}\n",
      "13:18:57.587 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20de0982{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "13:18:57.588 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@314b257f{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "13:18:57.592 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fb670bb{/static/sql,null,AVAILABLE,@Spark}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 13:18:58 WARNING mlflow.spark: With Pyspark >= 3.2, PYSPARK_PIN_THREAD environment variable must be set to false for Spark datasource autologging to work.\n",
      "2025/09/15 13:18:59 INFO mlflow.spark.autologging: Autologging successfully enabled for spark.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:19:00.088 [pool-30-thread-1] INFO  org.mlflow.spark.autologging.MlflowAutologEventPublisher$ - Subscriber with repl ID PythonSubscriber[/opt/spark/.venv/lib/python3.12/site-packages/ipykernel_launcher.py][860777c8a8db4ae8bf314e3fe748c970] not responding to health checks, removing it\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(4, \"spark i j k\"), (5, \"l m n\"), (6, \"spark hadoop spark\"), (7, \"apache hadoop\")],\n",
    "    [\"id\", \"text\"],\n",
    ")\n",
    "mlflow.set_tracking_uri(\"http://spark-master:3000\")\n",
    "mlflow.spark.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8455af0-cf58-49c0-9330-7e85a7412fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 13:19:04 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh(<full-path-to-git-executable>)\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial message can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|silent|none|n|0: for no message or exception\n",
      "    - warn|w|warning|log|l|1: for a warning message (logging level CRITICAL, displayed by default)\n",
      "    - error|e|exception|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:19:04.896 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 152.052768 ms\n",
      "13:19:04.947 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: toPandas at /tmp/ipykernel_1999/2692516043.py:2\n",
      "13:19:04.957 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (toPandas at /tmp/ipykernel_1999/2692516043.py:2) with 2 output partitions\n",
      "13:19:04.958 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (toPandas at /tmp/ipykernel_1999/2692516043.py:2)\n",
      "13:19:04.958 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "13:19:04.959 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "13:19:04.961 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[6] at toPandas at /tmp/ipykernel_1999/2692516043.py:2), which has no missing parents\n",
      "13:19:05.032 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 12.6 KiB, free 434.4 MiB)\n",
      "13:19:05.252 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 434.4 MiB)\n",
      "13:19:05.257 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on spark-master:7082 (size: 7.2 KiB, free: 434.4 MiB)\n",
      "13:19:05.261 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
      "13:19:05.272 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at toPandas at /tmp/ipykernel_1999/2692516043.py:2) (first 15 tasks are for partitions Vector(0, 1))\n",
      "13:19:05.274 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0\n",
      "13:19:05.281 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.FairSchedulableBuilder - Added task set TaskSet_0.0 tasks to pool default\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 0) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:19:06.106 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint - Executor added: app-20250915131845-0004/0 on worker-20250915120501-10.89.2.6-7080 (10.89.2.6:7080) with 1 core(s)\n",
      "13:19:06.108 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend - Granted executor ID app-20250915131845-0004/0 on hostPort 10.89.2.6:7080 with 1 core(s), 1024.0 MiB RAM\n",
      "13:19:06.109 [spark-dynamic-executor-allocation] INFO  org.apache.spark.ExecutorAllocationManager - Requesting 1 new executor because tasks are backlogged (new desired total will be 1 for resource profile id: 0)\n",
      "13:19:06.203 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint - Executor updated: app-20250915131845-0004/0 is now RUNNING\n",
      "13:19:07.032 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint - Executor added: app-20250915131845-0004/1 on worker-20250915120501-10.89.2.6-7080 (10.89.2.6:7080) with 1 core(s)\n",
      "13:19:07.033 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend - Granted executor ID app-20250915131845-0004/1 on hostPort 10.89.2.6:7080 with 1 core(s), 1024.0 MiB RAM\n",
      "13:19:07.033 [spark-dynamic-executor-allocation] INFO  org.apache.spark.ExecutorAllocationManager - Requesting 1 new executor because tasks are backlogged (new desired total will be 2 for resource profile id: 0)\n",
      "13:19:07.103 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint - Executor updated: app-20250915131845-0004/1 is now RUNNING\n",
      "13:19:08.909 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.89.2.6:34592) with ID 0,  ResourceProfileId 0\n",
      "13:19:08.938 [spark-listener-group-executorManagement] INFO  org.apache.spark.scheduler.dynalloc.ExecutorMonitor - New executor 0 has registered (new total is 1)\n",
      "13:19:09.082 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 10.89.2.6:7083 with 434.4 MiB RAM, BlockManagerId(0, 10.89.2.6, 7083, None)\n",
      "13:19:09.161 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (10.89.2.6, executor 0, partition 0, PROCESS_LOCAL, 8929 bytes) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:19:09.862 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 10.89.2.6:7083 (size: 7.2 KiB, free: 434.4 MiB)\n",
      "13:19:09.864 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.89.2.6:34598) with ID 1,  ResourceProfileId 0\n",
      "13:19:09.866 [spark-listener-group-executorManagement] INFO  org.apache.spark.scheduler.dynalloc.ExecutorMonitor - New executor 1 has registered (new total is 2)\n",
      "13:19:09.993 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 10.89.2.6:7084 with 434.4 MiB RAM, BlockManagerId(1, 10.89.2.6, 7084, None)\n",
      "13:19:10.056 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (10.89.2.6, executor 1, partition 1, PROCESS_LOCAL, 8944 bytes) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:19:11.154 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 10.89.2.6:7084 (size: 7.2 KiB, free: 434.4 MiB)\n",
      "13:19:12.353 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 3234 ms on 10.89.2.6 (executor 0) (1/2)\n",
      "13:19:12.367 [dag-scheduler-event-loop] INFO  org.apache.spark.api.python.PythonAccumulatorV2 - Connected to AccumulatorServer at host: 127.0.0.1 port: 49469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:=============================>                             (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:19:12.958 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 2903 ms on 10.89.2.6 (executor 1) (2/2)\n",
      "13:19:12.959 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool default\n",
      "13:19:12.959 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (toPandas at /tmp/ipykernel_1999/2692516043.py:2) finished in 7.984 s\n",
      "13:19:12.963 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "13:19:12.963 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished\n",
      "13:19:12.964 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: toPandas at /tmp/ipykernel_1999/2692516043.py:2, took 8.017093 s\n",
      "🏃 View run unequaled-shrimp-960 at: http://spark-master:3000/#/experiments/0/runs/cbaef4e2c346409e942afcd9ef8cfb38\n",
      "🧪 View experiment at: http://spark-master:3000/#/experiments/0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:20:12.469 [spark-dynamic-executor-allocation] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend - Requesting to kill executor(s) 0\n",
      "13:20:12.486 [spark-dynamic-executor-allocation] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend - Actual list of executor(s) to be killed is 0\n",
      "13:20:12.550 [spark-dynamic-executor-allocation] INFO  org.apache.spark.ExecutorAllocationManager - Executors 0 removed due to idle timeout.\n",
      "13:20:13.057 [spark-dynamic-executor-allocation] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend - Requesting to kill executor(s) 1\n",
      "13:20:13.058 [spark-dynamic-executor-allocation] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend - Actual list of executor(s) to be killed is 1\n",
      "13:20:13.079 [spark-dynamic-executor-allocation] INFO  org.apache.spark.ExecutorAllocationManager - Executors 1 removed due to idle timeout.\n",
      "13:20:17.678 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Executor 0 on 10.89.2.6 killed by driver.\n",
      "13:20:17.699 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Executor lost: 0 (epoch 0)\n",
      "13:20:17.702 [spark-listener-group-executorManagement] INFO  org.apache.spark.scheduler.dynalloc.ExecutorMonitor - Executor 0 is removed. Remove reason statistics: (gracefully decommissioned: 0, decommision unfinished: 0, driver killed: 1, unexpectedly exited: 0).\n",
      "13:20:17.707 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Trying to remove executor 0 from BlockManagerMaster.\n",
      "13:20:17.709 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Removing block manager BlockManagerId(0, 10.89.2.6, 7083, None)\n",
      "13:20:17.709 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.BlockManagerMaster - Removed 0 successfully in removeExecutor\n",
      "13:20:18.170 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Executor 1 on 10.89.2.6 killed by driver.\n",
      "13:20:18.171 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Executor lost: 1 (epoch 0)\n",
      "13:20:18.172 [spark-listener-group-executorManagement] INFO  org.apache.spark.scheduler.dynalloc.ExecutorMonitor - Executor 1 is removed. Remove reason statistics: (gracefully decommissioned: 0, decommision unfinished: 0, driver killed: 2, unexpectedly exited: 0).\n",
      "13:20:18.172 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Trying to remove executor 1 from BlockManagerMaster.\n",
      "13:20:18.172 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Removing block manager BlockManagerId(1, 10.89.2.6, 7084, None)\n",
      "13:20:18.172 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.BlockManagerMaster - Removed 1 successfully in removeExecutor\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run() as active_run:\n",
    "    pandas_df = df.toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
