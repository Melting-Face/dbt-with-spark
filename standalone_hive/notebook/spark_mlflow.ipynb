{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "647e39e2-e799-4f17-a135-8c53243be314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.spark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e652122-5615-4c0d-86d8-52d87f8f6c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Setting Spark log level to \"INFO\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:06:46.275 [Thread-3] INFO  org.apache.spark.resource.ResourceUtils - ==============================================================\n",
      "13:06:46.281 [Thread-3] INFO  org.apache.spark.resource.ResourceUtils - No custom resources configured for spark.driver.\n",
      "13:06:46.282 [Thread-3] INFO  org.apache.spark.resource.ResourceUtils - ==============================================================\n",
      "13:06:46.282 [Thread-3] INFO  org.apache.spark.SparkContext - Submitted application: Spark Mlflow\n",
      "13:06:46.323 [Thread-3] INFO  org.apache.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "13:06:46.332 [Thread-3] INFO  org.apache.spark.resource.ResourceProfile - Limiting resource is cpus at 1 tasks per executor\n",
      "13:06:46.334 [Thread-3] INFO  org.apache.spark.resource.ResourceProfileManager - Added ResourceProfile id: 0\n",
      "13:06:46.623 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing view acls to: root\n",
      "13:06:46.625 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing modify acls to: root\n",
      "13:06:46.627 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing view acls groups to: \n",
      "13:06:46.627 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing modify acls groups to: \n",
      "13:06:46.628 [Thread-3] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
      "13:06:46.994 [Thread-3] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7078. Attempting port 7079.\n",
      "13:06:46.998 [Thread-3] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7079. Attempting port 7080.\n",
      "13:06:47.002 [Thread-3] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7080. Attempting port 7081.\n",
      "13:06:47.015 [Thread-3] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 7081.\n",
      "13:06:47.059 [Thread-3] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker\n",
      "13:06:47.107 [Thread-3] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster\n",
      "13:06:47.129 [Thread-3] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "13:06:47.129 [Thread-3] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up\n",
      "13:06:47.136 [Thread-3] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat\n",
      "13:06:47.156 [Thread-3] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-c5317b00-e6c4-457e-ab67-19b0444b7545\n",
      "13:06:47.167 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 434.4 MiB\n",
      "13:06:47.185 [Thread-3] INFO  org.apache.spark.SparkEnv - Registering OutputCommitCoordinator\n",
      "13:06:47.293 [Thread-3] INFO  org.sparkproject.jetty.util.log - Logging initialized @4471ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "13:06:47.385 [Thread-3] INFO  org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "13:06:47.403 [Thread-3] INFO  org.sparkproject.jetty.server.Server - jetty-9.4.54.v20240208; built: 2024-02-08T19:42:39.027Z; git: cef3fbd6d736a21e7d541a5db490381d95a2047d; jvm 11.0.25+9\n",
      "13:06:47.448 [Thread-3] INFO  org.sparkproject.jetty.server.Server - Started @4630ms\n",
      "13:06:47.506 [Thread-3] WARN  org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "13:06:47.518 [Thread-3] INFO  org.sparkproject.jetty.server.AbstractConnector - Started ServerConnector@6ec8585{HTTP/1.1, (http/1.1)}{0.0.0.0:4041}\n",
      "13:06:47.518 [Thread-3] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4041.\n",
      "13:06:47.552 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@317af51c{/,null,AVAILABLE,@Spark}\n",
      "13:06:47.629 [Thread-3] INFO  org.apache.spark.scheduler.FairSchedulableBuilder - Fair scheduler configuration not found, created default pool: default, schedulingMode: FAIR, minShare: 0, weight: 1\n",
      "13:06:47.696 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint - Connecting to master spark://spark-master:7077...\n",
      "13:06:47.775 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory - Successfully created connection to spark-master/10.89.2.6:7077 after 49 ms (0 ms spent in bootstraps)\n",
      "13:06:48.045 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend - Connected to Spark cluster with app ID app-20250915130648-0001\n",
      "13:06:48.066 [Thread-3] WARN  org.apache.spark.util.Utils - Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 7079. Attempting port 7080.\n",
      "13:06:48.073 [Thread-3] WARN  org.apache.spark.util.Utils - Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 7080. Attempting port 7081.\n",
      "13:06:48.079 [Thread-3] WARN  org.apache.spark.util.Utils - Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 7081. Attempting port 7082.\n",
      "13:06:48.082 [Thread-3] INFO  org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7082.\n",
      "13:06:48.083 [Thread-3] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on spark-master 0.0.0.0:7082\n",
      "13:06:48.088 [Thread-3] INFO  org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "13:06:48.088 [Thread-3] INFO  org.apache.spark.storage.BlockManager - external shuffle service port = 7337\n",
      "13:06:48.106 [Thread-3] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, spark-master, 7082, None)\n",
      "13:06:48.112 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager spark-master:7082 with 434.4 MiB RAM, BlockManagerId(driver, spark-master, 7082, None)\n",
      "13:06:48.122 [Thread-3] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, spark-master, 7082, None)\n",
      "13:06:48.127 [Thread-3] INFO  org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, spark-master, 7082, None)\n",
      "13:06:48.767 [Thread-3] INFO  org.apache.spark.deploy.history.RollingEventLogFilesWriter - Logging events to file:/opt/spark/events/eventlog_v2_app-20250915130648-0001/events_1_app-20250915130648-0001.zstd\n",
      "13:06:48.993 [Thread-3] INFO  org.apache.spark.util.Utils - Using initial executors = 0, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "13:06:49.058 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@317af51c{/,null,STOPPED,@Spark}\n",
      "13:06:49.060 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21bc655{/jobs,null,AVAILABLE,@Spark}\n",
      "13:06:49.060 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@727c782a{/jobs/json,null,AVAILABLE,@Spark}\n",
      "13:06:49.061 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ac12f44{/jobs/job,null,AVAILABLE,@Spark}\n",
      "13:06:49.062 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c410528{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "13:06:49.062 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79db0aed{/stages,null,AVAILABLE,@Spark}\n",
      "13:06:49.063 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f5aeb83{/stages/json,null,AVAILABLE,@Spark}\n",
      "13:06:49.064 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@52eafd60{/stages/stage,null,AVAILABLE,@Spark}\n",
      "13:06:49.065 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3db49a78{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "13:06:49.066 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33212d90{/stages/pool,null,AVAILABLE,@Spark}\n",
      "13:06:49.066 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4adce4e4{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "13:06:49.067 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68a8838f{/storage,null,AVAILABLE,@Spark}\n",
      "13:06:49.068 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44861{/storage/json,null,AVAILABLE,@Spark}\n",
      "13:06:49.068 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@525236a{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "13:06:49.069 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@193b5c55{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "13:06:49.070 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@771fdc02{/environment,null,AVAILABLE,@Spark}\n",
      "13:06:49.071 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d72f236{/environment/json,null,AVAILABLE,@Spark}\n",
      "13:06:49.071 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@436dd325{/executors,null,AVAILABLE,@Spark}\n",
      "13:06:49.072 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5136df19{/executors/json,null,AVAILABLE,@Spark}\n",
      "13:06:49.073 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69ac3dfe{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "13:06:49.073 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e322255{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "13:06:49.074 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2bb4db3b{/executors/heapHistogram,null,AVAILABLE,@Spark}\n",
      "13:06:49.074 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d5b961{/executors/heapHistogram/json,null,AVAILABLE,@Spark}\n",
      "13:06:49.087 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5618d162{/static,null,AVAILABLE,@Spark}\n",
      "13:06:49.088 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3eae9fd{/,null,AVAILABLE,@Spark}\n",
      "13:06:49.090 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5aa72f5f{/api,null,AVAILABLE,@Spark}\n",
      "13:06:49.091 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@305c12a7{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "13:06:49.091 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@545dad49{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "13:06:49.095 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ab09767{/metrics/json,null,AVAILABLE,@Spark}\n",
      "13:06:49.095 [Thread-3] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Spark Mlflow\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd083e90-c831-4194-8c67-ee776503bd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:06:54.976 [Thread-3] INFO  org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "13:06:54.984 [Thread-3] INFO  org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/opt/spark/notebook/spark-warehouse'.\n",
      "13:06:55.014 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74e7c4c9{/SQL,null,AVAILABLE,@Spark}\n",
      "13:06:55.015 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@502c9ed1{/SQL/json,null,AVAILABLE,@Spark}\n",
      "13:06:55.016 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@eea466f{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "13:06:55.017 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12b4b89e{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "13:06:55.020 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77b54e72{/static/sql,null,AVAILABLE,@Spark}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 13:06:57 WARNING mlflow.spark: With Pyspark >= 3.2, PYSPARK_PIN_THREAD environment variable must be set to false for Spark datasource autologging to work.\n",
      "2025/09/15 13:06:57 INFO mlflow.spark.autologging: Autologging successfully enabled for spark.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:06:58.506 [pool-30-thread-1] INFO  org.mlflow.spark.autologging.MlflowAutologEventPublisher$ - Subscriber with repl ID PythonSubscriber[/opt/spark/.venv/lib/python3.12/site-packages/ipykernel_launcher.py][c5af1bbd18814d7883706d0e9969b89c] not responding to health checks, removing it\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(4, \"spark i j k\"), (5, \"l m n\"), (6, \"spark hadoop spark\"), (7, \"apache hadoop\")],\n",
    "    [\"id\", \"text\"],\n",
    ")\n",
    "df.write.csv(\"s3a://warehouse/test.csv\", header=True)\n",
    "mlflow.spark.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8455af0-cf58-49c0-9330-7e85a7412fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:07:59.738 [Thread-3] INFO  org.elasticsearch.hadoop.util.Version - Elasticsearch Hadoop v8.17.6 [22351502c0]\n",
      "13:07:59.923 [Thread-3] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig - Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "13:07:59.935 [Thread-3] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - Scheduled Metric snapshot period at 10 second(s).\n",
      "13:07:59.936 [Thread-3] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - s3a-file-system metrics system started\n",
      "13:08:01.411 [Thread-3] WARN  org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory - Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "13:08:01.412 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "13:08:01.412 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "13:08:01.413 [Thread-3] INFO  org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory - Using Committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202509151308013703279138509939700_0000}; taskId=attempt_202509151308013703279138509939700_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2bc0f76e}; outputPath=s3a://warehouse/test.csv, workPath=s3a://warehouse/test.csv/_temporary/0/_temporary/attempt_202509151308013703279138509939700_0000_m_000000_0, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false} for s3a://warehouse/test.csv\n",
      "13:08:01.414 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "13:08:01.889 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 266.805065 ms\n",
      "13:08:01.938 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: csv at <unknown>:0\n",
      "13:08:01.956 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (csv at <unknown>:0) with 2 output partitions\n",
      "13:08:01.957 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (csv at <unknown>:0)\n",
      "13:08:01.957 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "13:08:01.959 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "13:08:01.965 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[6] at csv at <unknown>:0), which has no missing parents\n",
      "13:08:02.212 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 230.3 KiB, free 434.2 MiB)\n",
      "13:08:02.501 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 110.6 KiB, free 434.1 MiB)\n",
      "13:08:02.506 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on spark-master:7082 (size: 110.6 KiB, free: 434.3 MiB)\n",
      "13:08:02.510 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
      "13:08:02.534 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at csv at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "13:08:02.536 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0\n",
      "13:08:02.556 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.FairSchedulableBuilder - Added task set TaskSet_0.0 tasks to pool default\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 0) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:08:03.182 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint - Executor added: app-20250915130648-0001/0 on worker-20250915120501-10.89.2.6-7080 (10.89.2.6:7080) with 1 core(s)\n",
      "13:08:03.183 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend - Granted executor ID app-20250915130648-0001/0 on hostPort 10.89.2.6:7080 with 1 core(s), 1024.0 MiB RAM\n",
      "13:08:03.184 [spark-dynamic-executor-allocation] INFO  org.apache.spark.ExecutorAllocationManager - Requesting 1 new executor because tasks are backlogged (new desired total will be 1 for resource profile id: 0)\n",
      "13:08:03.646 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint - Executor updated: app-20250915130648-0001/0 is now RUNNING\n",
      "13:08:04.100 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint - Executor added: app-20250915130648-0001/1 on worker-20250915120501-10.89.2.6-7080 (10.89.2.6:7080) with 1 core(s)\n",
      "13:08:04.101 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend - Granted executor ID app-20250915130648-0001/1 on hostPort 10.89.2.6:7080 with 1 core(s), 1024.0 MiB RAM\n",
      "13:08:04.103 [spark-dynamic-executor-allocation] INFO  org.apache.spark.ExecutorAllocationManager - Requesting 1 new executor because tasks are backlogged (new desired total will be 2 for resource profile id: 0)\n",
      "13:08:04.147 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint - Executor updated: app-20250915130648-0001/1 is now RUNNING\n",
      "13:08:07.815 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.89.2.6:52716) with ID 0,  ResourceProfileId 0\n",
      "13:08:07.829 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.89.2.6:52708) with ID 1,  ResourceProfileId 0\n",
      "13:08:07.846 [spark-listener-group-executorManagement] INFO  org.apache.spark.scheduler.dynalloc.ExecutorMonitor - New executor 0 has registered (new total is 1)\n",
      "13:08:07.847 [spark-listener-group-executorManagement] INFO  org.apache.spark.scheduler.dynalloc.ExecutorMonitor - New executor 1 has registered (new total is 2)\n",
      "13:08:08.009 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 10.89.2.6:7084 with 434.4 MiB RAM, BlockManagerId(1, 10.89.2.6, 7084, None)\n",
      "13:08:08.012 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 10.89.2.6:7083 with 434.4 MiB RAM, BlockManagerId(0, 10.89.2.6, 7083, None)\n",
      "13:08:08.099 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (10.89.2.6, executor 0, partition 0, PROCESS_LOCAL, 8929 bytes) \n",
      "13:08:08.117 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (10.89.2.6, executor 1, partition 1, PROCESS_LOCAL, 8944 bytes) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:08:08.775 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 10.89.2.6:7084 (size: 110.6 KiB, free: 434.3 MiB)\n",
      "13:08:08.776 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 10.89.2.6:7083 (size: 110.6 KiB, free: 434.3 MiB)\n",
      "13:08:15.287 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 7166 ms on 10.89.2.6 (executor 1) (1/2)\n",
      "13:08:15.301 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 7228 ms on 10.89.2.6 (executor 0) (2/2)\n",
      "13:08:15.304 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool default\n",
      "13:08:15.317 [dag-scheduler-event-loop] INFO  org.apache.spark.api.python.PythonAccumulatorV2 - Connected to AccumulatorServer at host: 127.0.0.1 port: 48777\n",
      "13:08:15.337 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (csv at <unknown>:0) finished in 13.343 s\n",
      "13:08:15.346 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "13:08:15.351 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished\n",
      "13:08:15.362 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: csv at <unknown>:0, took 13.423506 s\n",
      "13:08:15.380 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Start to commit write Job 36cbce52-de3a-4c6f-9e69-fb0f54011961.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:08:15.692 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on spark-master:7082 in memory (size: 110.6 KiB, free: 434.4 MiB)\n",
      "13:08:15.788 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 10.89.2.6:7084 in memory (size: 110.6 KiB, free: 434.4 MiB)\n",
      "13:08:15.793 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 10.89.2.6:7083 in memory (size: 110.6 KiB, free: 434.4 MiB)\n",
      "13:08:16.105 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job 36cbce52-de3a-4c6f-9e69-fb0f54011961 committed. Elapsed time: 722 ms.\n",
      "13:08:16.116 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job 36cbce52-de3a-4c6f-9e69-fb0f54011961.\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
